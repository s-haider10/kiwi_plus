{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1aca886b2dfc440cbdd4db30b87d2779": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_aff902829fb04582818f35ea061dcb9e",
              "IPY_MODEL_48a90a6390284506970068caae219b44",
              "IPY_MODEL_088d530acf754ddd90ca94e3fd2fe93f"
            ],
            "layout": "IPY_MODEL_d0fa879395b2416aba07e12653da2e36"
          }
        },
        "aff902829fb04582818f35ea061dcb9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5f55ac22cd484734bf6d2bff6fa5851d",
            "placeholder": "​",
            "style": "IPY_MODEL_59d9d63bfc86446dbd5908884efda9bf",
            "value": "Evaluating: 100%"
          }
        },
        "48a90a6390284506970068caae219b44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_43e1567ddf1a4ea29fb0612ffe50c386",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_92524034421344d6bd44c27b80a876f6",
            "value": 3
          }
        },
        "088d530acf754ddd90ca94e3fd2fe93f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_780642b7f8c44470a2e28fc2ee7c7fb6",
            "placeholder": "​",
            "style": "IPY_MODEL_455d735e0e874b99a69be5efbcad9ecf",
            "value": " 3/3 [00:39&lt;00:00, 14.04s/it]"
          }
        },
        "d0fa879395b2416aba07e12653da2e36": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f55ac22cd484734bf6d2bff6fa5851d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "59d9d63bfc86446dbd5908884efda9bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "43e1567ddf1a4ea29fb0612ffe50c386": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "92524034421344d6bd44c27b80a876f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "780642b7f8c44470a2e28fc2ee7c7fb6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "455d735e0e874b99a69be5efbcad9ecf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "RAG Pipeline\n"
      ],
      "metadata": {
        "id": "b6iqo1jPKKxf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install assemblyai openai pygame nltk yt-dlp transformers opencv-python-headless scenedetect chromadb torch pytesseract pydub numpy pandas matplotlib seaborn sentence-transformers tqdm edge-tts\n",
        "!sudo apt-get update\n",
        "!sudo apt-get install -y tesseract-ocr\n",
        "\n",
        "from yt_dlp import YoutubeDL\n",
        "import os\n",
        "import argparse\n",
        "import subprocess\n",
        "import cv2\n",
        "from scenedetect import detect, ContentDetector\n",
        "from scenedetect.video_manager import VideoManager\n",
        "from scenedetect.scene_manager import SceneManager\n",
        "from scenedetect.stats_manager import StatsManager\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "import torch\n",
        "import chromadb\n",
        "from chromadb.config import Settings\n",
        "import os\n",
        "import json\n",
        "import torch\n",
        "import pytesseract\n",
        "import re\n",
        "from datetime import timedelta\n",
        "from transformers import CLIPProcessor, CLIPModel, CLIPTokenizer\n",
        "import assemblyai as aai\n",
        "from pydub import AudioSegment\n",
        "import openai\n",
        "import numpy as np\n",
        "import ast\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import sentence_transformers\n",
        "from tqdm.autonotebook import tqdm, trange"
      ],
      "metadata": {
        "id": "uUvt9X5OKHq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "openai.api_key=\"YOUR-API-KEY\"\n",
        "aai.settings.api_key = \"YOUR-API-KEY\"\n",
        "# openai_client = OpenAI.api_key"
      ],
      "metadata": {
        "id": "qLLGWywiKHxW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_to_seconds(timestamp):\n",
        "    hours, minutes, seconds = map(int, timestamp.split(\":\"))\n",
        "    return timedelta(hours=hours, minutes=minutes, seconds=seconds).total_seconds()\n",
        "\n",
        "def generate_summary(text):\n",
        "    system_prompt = \"\"\"\n",
        "        Your job is to analyze a text snippet, and analyze the text content, summarizing it in one line. Highlight the main ideas, key terms, and concepts being discussed.\n",
        "        Without any additional text, return this summary.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-4\",  # Replace with your authorized model, e.g., gpt-3.5-turbo\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system_prompt},\n",
        "                {\"role\": \"user\", \"content\": text}\n",
        "            ],\n",
        "            max_tokens=100  # Adjust as needed\n",
        "        )\n",
        "        summary = response.choices[0].message.content\n",
        "        return summary.strip()\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating summary: {e}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "fp5SFH1mKH6I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_summaries_to_json(json_file):\n",
        "    with open(json_file, 'r') as f:\n",
        "        transcriptions = json.load(f)\n",
        "    for entry in transcriptions:\n",
        "        full_text = \" \".join([utterance[\"text\"] for utterance in entry[\"transcription\"]])\n",
        "        summary = generate_summary(full_text)\n",
        "        entry[\"summary\"] = summary\n",
        "    with open(json_file, 'w') as f:\n",
        "        json.dump(transcriptions, f, indent=4)\n",
        "    print(f\"Summaries have been added and the file is updated.\")\n",
        "\n",
        "def transcribe_audio_in_chunks(audio):\n",
        "    CHUNK_SIZE = 45\n",
        "    transcriptions = []\n",
        "    chunks_dir = \"audio_chunks\"\n",
        "    os.makedirs(chunks_dir, exist_ok=True)\n",
        "    num_chunks = len(audio) // (CHUNK_SIZE * 1000)\n",
        "    for i in range(num_chunks + 1):\n",
        "        start_time = i * CHUNK_SIZE * 1000\n",
        "        end_time = (i + 1) * CHUNK_SIZE * 1000\n",
        "        chunk = audio[start_time:end_time]\n",
        "        chunk_filename = os.path.join(chunks_dir, f\"chunk_{i}.wav\")\n",
        "        chunk.export(chunk_filename, format=\"wav\")\n",
        "        config = aai.TranscriptionConfig(\n",
        "            speech_model=aai.SpeechModel.best,\n",
        "            summarization=True,\n",
        "            sentiment_analysis=True,\n",
        "            entity_detection=True,\n",
        "            speaker_labels=True,\n",
        "            filter_profanity=True,\n",
        "            language_detection=True\n",
        "        )\n",
        "        transcriber = aai.Transcriber(config=config)\n",
        "        transcript = transcriber.transcribe(chunk_filename)\n",
        "\n",
        "        while transcript.status != aai.TranscriptStatus.completed:\n",
        "            transcript = transcriber.transcribe(chunk_filename)\n",
        "        if transcript.status == aai.TranscriptStatus.error:\n",
        "            print(f\"Error in chunk {i}: {transcript.error}\")\n",
        "        else:\n",
        "            transcriptions.append({\n",
        "                \"start_time\": str(timedelta(milliseconds=start_time)),\n",
        "                \"end_time\": str(timedelta(milliseconds=end_time)),\n",
        "                \"transcription\": [  # Store each speaker's utterance separately\n",
        "                    {\"speaker\": utterance.speaker, \"text\": utterance.text}\n",
        "                    for utterance in transcript.utterances\n",
        "                ]\n",
        "            })\n",
        "        os.remove(chunk_filename)\n",
        "    with open('transcriptions.json', 'w') as json_file:\n",
        "        json.dump(transcriptions, json_file, indent=4)\n",
        "    print(\"Transcription completed and saved to 'transcriptions.json'.\")"
      ],
      "metadata": {
        "id": "8JQI7H8vKH-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def perform_ocr(image_dir):\n",
        "\n",
        "    def load_timestamps(json_path):\n",
        "        try:\n",
        "            with open(json_path, 'r') as f:\n",
        "                return json.load(f)\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading timestamps from {json_path}: {e}\")\n",
        "            return {}\n",
        "\n",
        "    def perform_ocr_with_preprocessing(image_dir, output_json_path, timestamps, max_text_length=500, language='eng'):\n",
        "        ocr_results = []\n",
        "        for image_file in os.listdir(image_dir):\n",
        "            if image_file.endswith(\".png\"):\n",
        "                image_path = os.path.join(image_dir, image_file)\n",
        "                timestamp = timestamps.get(image_file, \"00:00:00\")\n",
        "                try:\n",
        "                    image = cv2.imread(image_path)\n",
        "                    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "                    _, thresh = cv2.threshold(gray, 150, 255, cv2.THRESH_BINARY_INV)\n",
        "                    text = pytesseract.image_to_string(thresh, lang=language).strip()\n",
        "                    if len(text) > max_text_length:\n",
        "                        text = text[:max_text_length] + \"...\"\n",
        "\n",
        "                    ocr_results.append({\n",
        "                        \"timestamp\": timestamp,\n",
        "                        \"file_name\": image_file,\n",
        "                        \"text\": text\n",
        "                    })\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing {image_file}: {e}\")\n",
        "        try:\n",
        "            with open(output_json_path, \"w\", encoding=\"utf-8\") as json_file:\n",
        "                json.dump(ocr_results, json_file, ensure_ascii=False, indent=4)\n",
        "            print(f\"OCR results saved to {output_json_path}\")\n",
        "        except Exception as json_error:\n",
        "            print(f\"Error saving JSON file: {json_error}\")\n",
        "        return ocr_results\n",
        "\n",
        "    timestamps_json_path = os.path.join(image_dir, 'timestamps.json')\n",
        "    output_json_path = 'ocr_results_with_timestamps_newest.json'\n",
        "    max_text_length = 50000\n",
        "    timestamps = load_timestamps(timestamps_json_path)\n",
        "    ocr_results = perform_ocr_with_preprocessing(image_dir, output_json_path, timestamps, max_text_length, language='eng')\n",
        "    return None"
      ],
      "metadata": {
        "id": "cdvhPi4ZKIEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scene_detection(video_path):\n",
        "    def format_timedelta(seconds):\n",
        "        hours, remainder = divmod(int(seconds), 3600)\n",
        "        minutes, seconds = divmod(remainder, 60)\n",
        "        return f\"{hours:02d}:{minutes:02d}:{seconds:02d}\"\n",
        "    video_path = \"downloaded_video.mp4\"\n",
        "    output_dir = \"scenes\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    video_manager = VideoManager([video_path])\n",
        "    stats_manager = StatsManager()\n",
        "    scene_manager = SceneManager(stats_manager)\n",
        "    scene_manager.add_detector(ContentDetector())\n",
        "    video_manager.set_downscale_factor()\n",
        "    video_manager.start()\n",
        "    scene_manager.detect_scenes(frame_source=video_manager)\n",
        "    scenes = scene_manager.get_scene_list()\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    timestamps = {}\n",
        "    for i, scene in enumerate(scenes):\n",
        "        scene_start_frame = scene[0].get_frames()\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, scene_start_frame)\n",
        "        ret, frame = cap.read()\n",
        "        if ret:\n",
        "            scene_filename = f\"scene_{i:03d}.png\"\n",
        "            cv2.imwrite(os.path.join(output_dir, scene_filename), frame)\n",
        "            timestamp = scene_start_frame / fps\n",
        "            timestamps[scene_filename] = format_timedelta(timestamp)\n",
        "    cap.release()\n",
        "    with open(os.path.join(output_dir, \"timestamps.json\"), \"w\") as f:\n",
        "        json.dump(timestamps, f, indent=4)\n",
        "    print(f\"Scene detection complete. Selected scenes saved in '{output_dir}' folder.\")\n",
        "    print(f\"Timestamps saved in '{output_dir}/timestamps.json'.\")"
      ],
      "metadata": {
        "id": "tUI9hglWKIK8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def download_video(url: str) -> str:\n",
        "    ydl_opts = {\n",
        "        'format': 'best[ext=mp4]',\n",
        "        'outtmpl': 'downloaded_video.mp4',\n",
        "    }\n",
        "    with YoutubeDL(ydl_opts) as ydl:\n",
        "        ydl.download([url])\n",
        "\n",
        "    video_path = 'downloaded_video.mp4'\n",
        "    print(f\"Video downloaded successfully: {video_path}\")\n",
        "    return video_path\n",
        "\n",
        "def extract_audio(video_path):\n",
        "    audio_path = 'extracted_audio.wav'\n",
        "    try:\n",
        "        subprocess.call(['ffmpeg', '-i', video_path, '-vn', '-acodec', 'pcm_s16le', '-ar', '44100', '-ac', '2', audio_path])\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"Error during audio extraction: {e}\")\n",
        "        return None\n",
        "    return audio_path"
      ],
      "metadata": {
        "id": "u6YgY1TDKISo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "video_url = input(\"Please enter the YouTube video URL: \").strip()"
      ],
      "metadata": {
        "id": "sJTGQ75bKIV0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2017f69-a0ed-49df-956d-da5c46efe12f"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Please enter the YouTube video URL: https://youtu.be/5sLYAQS9sWQ?feature=shared\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def run(video_url):\n",
        "    # video_path = download_video(video_url)\n",
        "\n",
        "    # audio_path = extract_audio(video_path)\n",
        "\n",
        "    # scene_detection(video_path)\n",
        "\n",
        "    # perform_ocr('scenes')\n",
        "\n",
        "    # audio = AudioSegment.from_wav(audio_path)\n",
        "\n",
        "    # transcribe_audio_in_chunks(audio)\n",
        "\n",
        "    add_summaries_to_json(\"transcriptions.json\")\n",
        "\n",
        "    with open('scenes/timestamps.json', 'r') as f:\n",
        "        timestamps_images = json.load(f)\n",
        "\n",
        "    timestamps_in_seconds = {image: convert_to_seconds(time) for image, time in timestamps_images.items()}\n",
        "\n",
        "    with open('transcriptions.json', 'r') as f:\n",
        "        transcription_data = json.load(f)\n",
        "\n",
        "    for segment in transcription_data:\n",
        "        start_time_sec = convert_to_seconds(segment[\"start_time\"])\n",
        "        end_time_sec = convert_to_seconds(segment[\"end_time\"])\n",
        "        image_names = [\n",
        "            image for image, time_sec in timestamps_in_seconds.items()\n",
        "            if start_time_sec <= time_sec <= end_time_sec\n",
        "        ]\n",
        "        segment[\"image_names\"] = image_names\n",
        "\n",
        "    with open('transcriptions.json', 'w') as f:\n",
        "        json.dump(transcription_data, f, indent=4)\n",
        "\n",
        "    transcription_data = 'transcriptions.json'\n",
        "\n",
        "    client_capstone = chromadb.PersistentClient(path=\"chromadb\")\n",
        "    collection_transcriptions_clip = client_capstone.create_collection(name=\"Capstone_Kiwi_clip\")\n",
        "    collection_transcriptions_bert = client_capstone.create_collection(name=\"Capstone_Kiwi_bert\")\n",
        "\n",
        "    with open(transcription_data, 'r') as file:\n",
        "        transcription_data = json.load(file)\n",
        "    data = transcription_data\n",
        "    model_name = \"openai/clip-vit-base-patch32\"\n",
        "\n",
        "\n",
        "    clip_model = CLIPModel.from_pretrained(model_name)\n",
        "    clip_tokenizer = CLIPTokenizer.from_pretrained(model_name)\n",
        "    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = clip_model.to(device)\n",
        "\n",
        "\n",
        "    bert_model_name = \"bert-base-uncased\"\n",
        "    bert_model = AutoModel.from_pretrained(bert_model_name)\n",
        "    bert_tokenizer = AutoTokenizer.from_pretrained(bert_model_name)\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    bert_model = bert_model.to(device)\n",
        "\n",
        "    def embed_text_bert(text):\n",
        "        inputs = bert_tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
        "        with torch.no_grad():\n",
        "            outputs = bert_model(**inputs)\n",
        "            text_embedding = outputs.last_hidden_state[:, 0, :]\n",
        "        return text_embedding.squeeze().cpu().numpy()\n",
        "\n",
        "    def embed_text(text):\n",
        "        text_inputs = processor(text=[text], return_tensors=\"pt\", padding=True).to(device)\n",
        "        with torch.no_grad():\n",
        "            text_embeddings = clip_model.get_text_features(**text_inputs)\n",
        "        return text_embeddings.squeeze().cpu().numpy()\n",
        "\n",
        "    embeddings = []\n",
        "    for item in transcription_data:\n",
        "        summary_text = item['summary']\n",
        "        embedding = embed_text(summary_text)\n",
        "        embeddings.append(embedding)\n",
        "\n",
        "    for i, item in enumerate(transcription_data):\n",
        "        summary_text = item['summary']\n",
        "        collection_transcriptions_clip.add(\n",
        "            documents=str(item),\n",
        "            embeddings=[embeddings[i]],\n",
        "            metadatas=None,\n",
        "            ids=[str(i)]\n",
        "        )\n",
        "\n",
        "    bert_embeddings = []\n",
        "    for item in transcription_data:\n",
        "        summary_text = item['summary']\n",
        "        embedding = embed_text_bert(summary_text)\n",
        "        bert_embeddings.append(embedding)\n",
        "\n",
        "    for i, item in enumerate(transcription_data):\n",
        "        collection_transcriptions_bert.add(\n",
        "            documents=str(item),\n",
        "            embeddings=[bert_embeddings[i]],\n",
        "            metadatas=None,\n",
        "            ids=[str(i)]\n",
        "        )\n",
        "\n",
        "    print(\"Data successfully added to collection_transcriptions.\")\n",
        "    print('Now we can initiate our audio LLM pipeline.')\n",
        "\n",
        "\n",
        "run(video_url)"
      ],
      "metadata": {
        "id": "1ksu2231KIy9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initiating the Chat Process (Audio/Text Mode)"
      ],
      "metadata": {
        "id": "lVRlcRfpLKyg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import assemblyai as aai\n",
        "from openai import OpenAI\n",
        "import time\n",
        "import os\n",
        "import pygame\n",
        "import threading\n",
        "import atexit\n",
        "import asyncio\n",
        "import edge_tts\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"wordnet\")\n",
        "from yt_dlp import YoutubeDL\n",
        "import os\n",
        "import argparse\n",
        "import subprocess\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "import cv2\n",
        "from scenedetect import detect, ContentDetector\n",
        "from scenedetect.video_manager import VideoManager\n",
        "from scenedetect.scene_manager import SceneManager\n",
        "from scenedetect.stats_manager import StatsManager\n",
        "import chromadb\n",
        "from chromadb.config import Settings\n",
        "import os\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "import json\n",
        "import torch\n",
        "import pytesseract\n",
        "import re\n",
        "from datetime import timedelta\n",
        "from transformers import CLIPProcessor, CLIPModel, CLIPTokenizer\n",
        "import assemblyai as aai\n",
        "from pydub import AudioSegment\n",
        "from openai import OpenAI\n",
        "import numpy as np\n",
        "import ast\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import sentence_transformers\n",
        "from tqdm.autonotebook import tqdm, trange\n"
      ],
      "metadata": {
        "id": "Yi3sG6ANKJEc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AI_Assistant:\n",
        "    def __init__(self):\n",
        "        # add the os.env\n",
        "        aai.settings.api_key = \"<Your-API-Key>\"\n",
        "        self.openai_client = OpenAI(api_key=\"<Your-API-Key>\")\n",
        "        self.transcriber = None\n",
        "        self.full_transcript = []\n",
        "        nltk.download('stopwords')\n",
        "        nltk.download('punkt_tab')\n",
        "        self.stop_words = set(stopwords.words(\"english\"))\n",
        "        self.client_capstone = chromadb.PersistentClient(path=\"chromadb\")\n",
        "        self.collection_transcriptions_clip = self.client_capstone.get_collection(name=\"Capstone_Kiwi_clip\")\n",
        "        self.collection_transcriptions_bert = self.client_capstone.get_collection(name=\"Capstone_Kiwi_bert\")\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(self.device)\n",
        "        self.clip_tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "        self.processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
        "        self.bert_model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
        "        self.bert_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "        self.initialize_system_prompt()\n",
        "        atexit.register(self.cleanup)\n",
        "\n",
        "    def embed_text(self, text):\n",
        "        text_inputs = self.processor(text=[text], return_tensors=\"pt\", padding=True).to(self.device)\n",
        "        with torch.no_grad():\n",
        "            text_embeddings = self.clip_model.get_text_features(**text_inputs)\n",
        "        return text_embeddings.squeeze().cpu().numpy()\n",
        "\n",
        "    def embed_text_bert(self, text):\n",
        "        inputs = self.bert_tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True).to(self.device)\n",
        "        with torch.no_grad():\n",
        "            outputs = self.bert_model(**inputs)\n",
        "            text_embedding = outputs.last_hidden_state[:, 0, :]\n",
        "        return text_embedding.squeeze().cpu().numpy()\n",
        "\n",
        "    def get_relevant_context(self, query_text):\n",
        "        merged_info = \"\"\n",
        "        query_embedding = self.embed_text(query_text)\n",
        "        results = self.collection_transcriptions_clip.query(query_embeddings=query_embedding, n_results=3)\n",
        "        data_string = results['documents'][0]\n",
        "        for i, item in enumerate(data_string):\n",
        "            merged_info += f\"Context {i+1}: \\n\\n\"\n",
        "            merged_info += f\"\\n\\n{item}\\n\\n\"\n",
        "        return merged_info\n",
        "\n",
        "    def initialize_system_prompt(self):\n",
        "        system_prompt = {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": (\n",
        "                \"You are an AI teacher designed to assist students in understanding video content. \"\n",
        "                \"When a student asks a question, analyze the provided context from the video transcription carefully \"\n",
        "                \"and respond in a clear, engaging, and student-friendly manner. Match the language style and tone of the video \"\n",
        "                \"to maintain a cohesive learning experience. Your answers should be concise if the query requires a direct response, \"\n",
        "                \"limited to 2-3 sentences. For more complex or detailed questions, provide thorough explanations in 5-6 sentences, \"\n",
        "                \"incorporating examples, quotes, or key points from the video transcription where relevant. \"\n",
        "                \"Always aim to cite the exact timestamps (start and end) from the video to guide students to specific sections for additional clarity. \"\n",
        "                \"Use phrases like 'as explained in the video,' 'as mentioned around,' or 'as demonstrated in the example at' to cite the timestamps and \"\n",
        "                \"to help connect the response to the video content directly. Encourage students to revisit the video sections for a deeper understanding. \"\n",
        "                \"Ensure your responses are tailored to the context of the video and focus on promoting an interactive and supportive learning environment.\"\n",
        "                \"Without any additonal text, return this answer.\"\n",
        "            ),\n",
        "        }\n",
        "        self.full_transcript.append(system_prompt)\n",
        "\n",
        "    def preprocess_question(self, question):\n",
        "        tokens = word_tokenize(question.lower())\n",
        "        filtered_tokens = [word for word in tokens if word not in self.stop_words]\n",
        "        return ' '.join(filtered_tokens)\n",
        "\n",
        "    def start_transcription(self):\n",
        "        self.transcriber = aai.RealtimeTranscriber(\n",
        "            sample_rate=16000,\n",
        "            on_data=self.on_data,\n",
        "            on_error=self.on_error,\n",
        "            on_open=self.on_open,\n",
        "            on_close=self.on_close,\n",
        "            end_utterance_silence_threshold=1000\n",
        "        )\n",
        "        self.transcriber.connect()\n",
        "        microphone_stream = aai.extras.MicrophoneStream(sample_rate=16000)\n",
        "        self.transcriber.stream(microphone_stream)\n",
        "\n",
        "    def stop_transcription(self):\n",
        "        if self.transcriber:\n",
        "            self.transcriber.close()\n",
        "            self.transcriber = None\n",
        "\n",
        "    def on_open(self, session_opened: aai.RealtimeSessionOpened):\n",
        "        print(\"Please speak now...\")\n",
        "        return\n",
        "\n",
        "    def on_data(self, transcript: aai.RealtimeTranscript):\n",
        "        if not transcript.text:\n",
        "            return\n",
        "        if isinstance(transcript, aai.RealtimeFinalTranscript):\n",
        "            self.generate_ai_response(transcript)\n",
        "\n",
        "    def on_error(self, error: aai.RealtimeError):\n",
        "        print(\"An error occurred:\", error)\n",
        "        return\n",
        "\n",
        "    def on_close(self):\n",
        "        return\n",
        "\n",
        "    def generate_summary(self):\n",
        "        try:\n",
        "            with open('transcriptions.json', 'r') as file:\n",
        "                transcriptions_data = json.load(file)\n",
        "            summary_texts = []\n",
        "            for entry in transcriptions_data:\n",
        "                summary_texts.append(entry.get('summary', 'No summary available'))\n",
        "            aggregated_summary = \" \".join(summary_texts)\n",
        "\n",
        "            prompt = (\n",
        "                f\"Please summarize the following information into two clear and concise paragraphs. The information is a from a video that the user is watching, and learning from. \"\n",
        "                f\"The summary should highlight the key concepts, include important details, and provide any relevant examples to help the user understand. \"\n",
        "                f\"Make sure the content is easy to follow and captures the main points. \\n\\n\"\n",
        "                f\"{aggregated_summary}\"\n",
        "            )\n",
        "            messages_to_send = [{\"role\": \"user\", \"content\": prompt}]\n",
        "            response = self.openai_client.chat.completions.create(\n",
        "                model=\"gpt-4o-mini\",\n",
        "                messages=messages_to_send,\n",
        "                max_tokens=10000\n",
        "            )\n",
        "\n",
        "            ai_response = response.choices[0].message.content\n",
        "            print(\"\\nAI Summary Response:\\n\", ai_response)\n",
        "            self.prompt_user()\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred: {e}\")\n",
        "\n",
        "\n",
        "    def generate_ai_response(self, transcript, mode=\"audio\"):\n",
        "        self.stop_transcription()\n",
        "        user_text = transcript.text\n",
        "        processed_question = self.preprocess_question(user_text)\n",
        "        relevant_context = self.get_relevant_context(processed_question)\n",
        "        final_prompt = (\n",
        "        f\"This is the user question:\\n{user_text}\\n\\n\"\n",
        "        f\"These are the relevant contexts in order of relevancy:\\n{relevant_context}\\n\\n\"\n",
        "        \"Use the provided context to craft a well-structured, engaging, and listener-friendly response to the question. \"\n",
        "        \"Ensure your answer is accurate and uses examples, references, or quotes from the context to provide clarity and depth. \"\n",
        "        \"If possible, highlight specific sections or details from the context, using timestamps or references to guide the listener back to the source material. \"\n",
        "        \"Always include timestamps from the provided context if they are available, and mention them explicitly in your response as it would help the student learn better.\"\n",
        "        \"For example, include phrases like 'as mentioned in the context,' 'as explained in the section around,' or 'as demonstrated in the example provided.' \"\n",
        "        \"If the context does not fully address the question, rely on your knowledge to give a concise, efficient answer that remains accurate and informative. \"\n",
        "        \"After completing the response, provide the YouTube link for the relevant timestamp as a separate line in the following format: \"\n",
        "        \"'Relevant section: https://youtu.be/zizonToFXDs?si=AEz6grjxNfZ_ZT1x&t=XXX' where 'XXX' is the most relevant timestamp in seconds. \"\n",
        "        \"Prioritize clarity and engagement in your response, tailoring it to ensure it fits the tone and style expected by the audience. \"\n",
        "        \"If the question is something which is completely out of context, and has no relevance to the main ideas of the video, in that case \"\n",
        "        \"don't give any timestamps and the links from the video as it would not make sense. Just answer the question in a general way, and ask the user to ask questions about the video\"\n",
        "        \"in a more direct way.\"\n",
        "        \"Return only the answer and the link with no additional commentary or text.\"\n",
        "        )\n",
        "        self.full_transcript.append({\"role\": \"user\", \"content\": user_text})\n",
        "        messages_to_send = [\n",
        "            self.full_transcript[0],\n",
        "            {\"role\": \"user\", \"content\": final_prompt},\n",
        "        ]\n",
        "        response = self.openai_client.chat.completions.create(\n",
        "            model=\"gpt-4o-mini\",\n",
        "            messages=messages_to_send,\n",
        "            max_tokens=10000\n",
        "        )\n",
        "        ai_response = response.choices[0].message.content\n",
        "        self.full_transcript.append({\"role\": \"assistant\", \"content\": ai_response})\n",
        "        if mode == \"audio\":\n",
        "            asyncio.run(self.generate_audio(ai_response))\n",
        "        else:\n",
        "            print(f\"\\nAI Teacher: {ai_response}\")\n",
        "            self.prompt_user()\n",
        "\n",
        "    async def generate_audio(self, text):\n",
        "        print(f\"\\nAI Teacher: {text}\")\n",
        "        communicate = edge_tts.Communicate(text, \"en-AU-NatashaNeural\", rate=\"+20%\")\n",
        "        await communicate.save(\"response.mp3\")\n",
        "        pygame.mixer.init()\n",
        "        pygame.mixer.music.load(\"response.mp3\")\n",
        "        pygame.mixer.music.play()\n",
        "        while pygame.mixer.music.get_busy():\n",
        "            pygame.time.Clock().tick(10)\n",
        "        os.remove(\"response.mp3\")\n",
        "        self.prompt_user()\n",
        "\n",
        "\n",
        "    def prompt_user(self):\n",
        "        try:\n",
        "            user_input = input(\n",
        "                \"\\nEnter 1 to ask a question (voice mode), 2 to ask a question in text-mode, 3 to get a summary of the conversation/video, or 0 to finish the conversation: \")\n",
        "            if user_input == \"1\":\n",
        "                self.start_transcription()\n",
        "            elif user_input == \"2\":\n",
        "                question = input(\"\\nPlease type your question: \")\n",
        "                transcript = type(\"Transcript\", (object,), {\"text\": question})()\n",
        "                self.generate_ai_response(transcript, mode=\"text\")\n",
        "            elif user_input == \"3\":\n",
        "                print(\"\\nGenerating summary of the conversation...\\n\")\n",
        "                self.generate_summary()\n",
        "            elif user_input == \"0\":\n",
        "                print(\"\\nConversation Transcript: \\n\\n\")\n",
        "                self.full_transcript.pop(0)\n",
        "                for entry in self.full_transcript:\n",
        "                    print(f\"{entry['role'].capitalize()}: {entry['content']}\\n\\n\")\n",
        "                print(\"\\nThank you for the conversation!\\n\")\n",
        "                self.cleanup()\n",
        "                os._exit(1)\n",
        "            else:\n",
        "                print(\"Invalid input. Please enter 1, 2, or 0.\")\n",
        "                self.prompt_user()\n",
        "        except KeyboardInterrupt:\n",
        "            self.cleanup()\n",
        "            print(\"\\nProgram interrupted. Exiting.\")\n",
        "            os._exit(1)\n",
        "\n",
        "    def cleanup(self):\n",
        "        self.stop_transcription()\n",
        "        os._exit(1)\n",
        "\n",
        "greeting = \"Hey! How are you doing, How can I help you?\"\n",
        "ai_assistant = AI_Assistant()\n",
        "asyncio.run(ai_assistant.generate_audio(greeting))"
      ],
      "metadata": {
        "id": "8rxIlOLpKJIL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-openai\n",
        "!pip install ragas"
      ],
      "metadata": {
        "id": "grUxtCtDxI0X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation"
      ],
      "metadata": {
        "id": "_CsBrVYt_WYp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = [\n",
        "  {\n",
        "\t  \"user_input\":\"How can LLMs be compared to the analogy of training a dog through commands?\",\n",
        "\t\t\"reference\":\"Large language models are general-purpose AI designed through pre-training and fine-tuning processes, analogous to training a dog with basic commands followed by specialized training for specific tasks.\",\n",
        "\t\t\"retrieved_contexts\":[\"Large language models are general-purpose AI designed through pre-training and fine-tuning processes, analogous to training a dog with basic commands followed by specialized training for specific tasks.\",\"The text discusses the advantages of using large language models (LLMs) for dialog applications, emphasizing their ability to handle extensive data without requiring expertise or training examples, focusing instead on effective prompt design.\",\"Large language models are pre-trained on extensive datasets for general purposes and then fine-tuned on smaller datasets for specific tasks, allowing versatile applications and minimal training requirements.\"],\n",
        "\t\t\"response\": \"Large language models (LLMs) can be compared to training a dog using commands because both processes involve a foundational stage followed by specialized training. As explained in the video, training a dog initially involves teaching basic commands like sit and stay, which equip the dog to interact well in everyday life. Similarly, LLMs undergo pre-training on large datasets to grasp general language functionalities before being fine-tuned for specific tasks, akin to how a service dog might receive additional specialized training for particular roles (0:00:45 - 0:01:30). This process ensures that LLMs are versatile, much like a well-trained dog that is capable of performing various tasks with minimal further instruction.\",\n",
        "\t},\n",
        "  {\n",
        "    \"user_input\": \"How are large language models (LLMs) related to deep learning?\",\n",
        "    \"reference\": \"The text introduces a course on Large Language Models (LLMs) conducted by John Ewald at Google Cloud, covering definitions, use cases, prompt tuning, and Google's Genai tools, while linking LLMs to deep learning and generative AI.\",\n",
        "\t\t\"retrieved_contexts\": [\"Large language models are pre-trained on extensive datasets for general purposes and then fine-tuned on smaller datasets for specific tasks, allowing versatile applications and minimal training requirements.', 'image_names'\",\"Large language models are versatile AI systems designed to address various language tasks and can be specialized for specific industries by utilizing extensive training data and high parameter counts.\",\"The text discusses the distinction between parameters and hyperparameters in machine learning, emphasizing that models trained on common human language tasks can serve as foundational tools for others, particularly due to resource constraints in developing large language models.\"],\n",
        "\t\t\"response\":\"Large language models (LLMs) are deeply connected to deep learning as they are fundamentally based on neural network architectures that learn from extensive datasets. As explained in the video, LLMs utilize a large amount of training data—often up to petabytes—and can have billions of parameters which equip them to tackle various language tasks like translation and text classification (Context 1, 0:03:00 - 0:03:45). The deep learning framework enables these models to pre-train on general language tasks and then be fine-tuned for specific applications, maximizing their versatility and efficiency.\"\n",
        "  },\n",
        "  {\n",
        "    \"user_input\": \"What makes large language models versatile?\",\n",
        "    \"reference\": \"Large language models are versatile AI systems designed to address various language tasks and can be specialized for specific industries by utilizing extensive training data and high parameter counts.\",\n",
        "\t\t\"retrieved_contexts\": [\"Large language models are pre-trained on extensive datasets for general purposes and then fine-tuned on smaller datasets for specific tasks, allowing versatile applications and minimal training requirements.\", \"Prompt design is crucial for all models, while prompt engineering is specific to high-performance systems, with three types of large language models (generic, instruction tuned, dialogue tuned) that require different prompting approaches.\", \"Large language models are versatile AI systems designed to address various language tasks and can be specialized for specific industries by utilizing extensive training data and high parameter counts.\"],\n",
        "    \"response\": \"Large language models are versatile because they are initially pre-trained on massive datasets for general use and then fine-tuned on smaller, task-specific datasets. This allows a single model to perform a wide range of tasks such as language translation, text classification, and question answering with minimal additional training, as highlighted around 0:03:00 to 0:03:45. Their ability to adapt to both general and specific tasks makes them immensely valuable across various industries.\",\n",
        "  },\n",
        "  {\n",
        "    \"user_input\": \"What system does Palm use for multi-task training?\",\n",
        "    \"reference\": \"Palm is a 540 billion parameter dense decoder-only transformer model, utilizing Google's Pathways system for efficient mul3ti-task training and improved understanding through distributed computation.\",\n",
        "    \"retrieved_contexts\": [\"Large language models excel in few-shot and zero-shot learning scenarios, demonstrating increasing performance with more data and parameters, exemplified by Google's 540 billion parameter Palm model.\",\"Palm is a 540 billion parameter dense decoder-only transformer model, utilizing Google's Pathways system for efficient multi-task training and improved understanding through distributed computation.\",\"The text describes the integration of Palm API with Maker Suite, which provides tools for model training, deployment, and monitoring of machine learning models through a user-friendly interface.\"],\n",
        "    \"response\":\"Palm uses Google's Pathways system for multi-task training, as mentioned around 0:04:30. This system allows Palm, a 540 billion parameter dense decoder-only transformer model, to efficiently train across multiple TPU V4 pods and manage various tasks simultaneously, improving its understanding of the world. The Pathways architecture enhances the model's ability to learn new tasks quickly and orchestrates distributed computation\"\n",
        "  },\n",
        "  {   \"user_input\": \"What features make Google’s Palm API user-friendly for developers?\",\n",
        "      \"reference\": \"The text describes the integration of Palm API with Maker Suite, which provides tools for model training, deployment, and monitoring of machine learning models through a user-friendly interface.\",\n",
        "      \"retrieved_contexts\": [\"Palm is a 540 billion parameter dense decoder-only transformer model, utilizing Google's Pathways system for efficient multi-task training and improved understanding through distributed computation.\", \"The text describes the integration of Palm API with Maker Suite, which provides tools for model training, deployment, and monitoring of machine learning models through a user-friendly interface.\",\"Large language models excel in few-shot and zero-shot learning scenarios, demonstrating increasing performance with more data and parameters, exemplified by Google's 540 billion parameter Palm model.\"],\n",
        "      \"response\":\"Google’s Palm API is user-friendly for developers primarily due to its integration with Maker Suite, as mentioned around 0:15:00 to 0:15:45. This integration provides a graphical user interface and various tools, including model training, deployment, and monitoring. The model training tool allows developers to train models using different algorithms, while the deployment tool offers various options to put models into production. Additionally, the monitoring tool provides performance insights through a user-friendly dashboard, making the entire process straightforward and efficient.\"\n",
        "  }\n",
        "]\n",
        "\n",
        "\n",
        "from ragas import EvaluationDataset\n",
        "eval_dataset = EvaluationDataset.from_list(data)\n",
        "from ragas.metrics import Faithfulness, FactualCorrectness, SemanticSimilarity\n",
        "from ragas import evaluate\n",
        "import os\n",
        "from ragas.llms import LangchainLLMWrapper\n",
        "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
        "from langchain_openai import ChatOpenAI\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"YOUR-API-KEY\"\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4\"))\n",
        "evaluator_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings())\n",
        "metrics = [\n",
        "    # LLMContextRecall(llm=evaluator_llm),\n",
        "    FactualCorrectness(mode = \"precision\", llm=evaluator_llm),\n",
        "    Faithfulness(llm=evaluator_llm),\n",
        "    SemanticSimilarity(embeddings=evaluator_embeddings)\n",
        "]\n",
        "\n",
        "results = evaluate(dataset=eval_dataset, metrics=metrics)\n",
        "\n",
        "df = results.to_pandas()\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292,
          "referenced_widgets": [
            "1aca886b2dfc440cbdd4db30b87d2779",
            "aff902829fb04582818f35ea061dcb9e",
            "48a90a6390284506970068caae219b44",
            "088d530acf754ddd90ca94e3fd2fe93f",
            "d0fa879395b2416aba07e12653da2e36",
            "5f55ac22cd484734bf6d2bff6fa5851d",
            "59d9d63bfc86446dbd5908884efda9bf",
            "43e1567ddf1a4ea29fb0612ffe50c386",
            "92524034421344d6bd44c27b80a876f6",
            "780642b7f8c44470a2e28fc2ee7c7fb6",
            "455d735e0e874b99a69be5efbcad9ecf"
          ]
        },
        "id": "eBRoF0bgcgIv",
        "outputId": "9e05ef64-c746-42cd-e7c6-1a647ec76b2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluating:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1aca886b2dfc440cbdd4db30b87d2779"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                          user_input  \\\n",
            "0  What features make Google’s Palm API user-frie...   \n",
            "\n",
            "                                  retrieved_contexts  \\\n",
            "0  [Palm is a 540 billion parameter dense decoder...   \n",
            "\n",
            "                                            response  \\\n",
            "0  Google’s Palm API is user-friendly for develop...   \n",
            "\n",
            "                                           reference  factual_correctness  \\\n",
            "0  The text describes the integration of Palm API...                 0.18   \n",
            "\n",
            "   faithfulness  semantic_similarity  \n",
            "0      0.181818             0.944585  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "oQF2rHIlyoMJ",
        "outputId": "26a1b135-56e0-4bcd-9af0-fea3cd87d4f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                          user_input  \\\n",
              "0  How can LLMs be compared to the analogy of tra...   \n",
              "1  How are large language models (LLMs) related t...   \n",
              "2        What makes large language models versatile?   \n",
              "3  What system does Palm use for multi-task train...   \n",
              "4  What features make Google’s Palm API user-frie...   \n",
              "\n",
              "                                  retrieved_contexts  \\\n",
              "0  [Large language models are general-purpose AI ...   \n",
              "1  [Large language models are pre-trained on exte...   \n",
              "2  [Large language models are pre-trained on exte...   \n",
              "3  [Large language models excel in few-shot and z...   \n",
              "4  [Palm is a 540 billion parameter dense decoder...   \n",
              "\n",
              "                                            response  \\\n",
              "0  Large language models (LLMs) can be compared t...   \n",
              "1  Large language models (LLMs) are deeply connec...   \n",
              "2  Large language models are versatile because th...   \n",
              "3  Palm uses Google's Pathways system for multi-t...   \n",
              "4  Google’s Palm API is user-friendly for develop...   \n",
              "\n",
              "                                           reference  factual_correctness  \\\n",
              "0  Large language models are general-purpose AI d...                 0.33   \n",
              "1  The text introduces a course on Large Language...                 0.09   \n",
              "2  Large language models are versatile AI systems...                 0.43   \n",
              "3  Palm is a 540 billion parameter dense decoder-...                 0.57   \n",
              "4  Palm is a 540 billion parameter dense decoder-...                 0.00   \n",
              "\n",
              "   faithfulness  semantic_similarity  \n",
              "0      0.555556             0.926641  \n",
              "1      0.181818             0.900435  \n",
              "2      0.555556             0.941025  \n",
              "3      0.500000             0.952131  \n",
              "4      0.181818             0.847082  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6c459026-bc44-460d-9a6e-8775ac9da740\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_input</th>\n",
              "      <th>retrieved_contexts</th>\n",
              "      <th>response</th>\n",
              "      <th>reference</th>\n",
              "      <th>factual_correctness</th>\n",
              "      <th>faithfulness</th>\n",
              "      <th>semantic_similarity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>How can LLMs be compared to the analogy of tra...</td>\n",
              "      <td>[Large language models are general-purpose AI ...</td>\n",
              "      <td>Large language models (LLMs) can be compared t...</td>\n",
              "      <td>Large language models are general-purpose AI d...</td>\n",
              "      <td>0.33</td>\n",
              "      <td>0.555556</td>\n",
              "      <td>0.926641</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>How are large language models (LLMs) related t...</td>\n",
              "      <td>[Large language models are pre-trained on exte...</td>\n",
              "      <td>Large language models (LLMs) are deeply connec...</td>\n",
              "      <td>The text introduces a course on Large Language...</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.181818</td>\n",
              "      <td>0.900435</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>What makes large language models versatile?</td>\n",
              "      <td>[Large language models are pre-trained on exte...</td>\n",
              "      <td>Large language models are versatile because th...</td>\n",
              "      <td>Large language models are versatile AI systems...</td>\n",
              "      <td>0.43</td>\n",
              "      <td>0.555556</td>\n",
              "      <td>0.941025</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>What system does Palm use for multi-task train...</td>\n",
              "      <td>[Large language models excel in few-shot and z...</td>\n",
              "      <td>Palm uses Google's Pathways system for multi-t...</td>\n",
              "      <td>Palm is a 540 billion parameter dense decoder-...</td>\n",
              "      <td>0.57</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.952131</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>What features make Google’s Palm API user-frie...</td>\n",
              "      <td>[Palm is a 540 billion parameter dense decoder...</td>\n",
              "      <td>Google’s Palm API is user-friendly for develop...</td>\n",
              "      <td>Palm is a 540 billion parameter dense decoder-...</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.181818</td>\n",
              "      <td>0.847082</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6c459026-bc44-460d-9a6e-8775ac9da740')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-6c459026-bc44-460d-9a6e-8775ac9da740 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-6c459026-bc44-460d-9a6e-8775ac9da740');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-e937d448-a343-4cc6-88c2-bbe72e584c21\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e937d448-a343-4cc6-88c2-bbe72e584c21')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-e937d448-a343-4cc6-88c2-bbe72e584c21 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"user_input\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"How are large language models (LLMs) related to deep learning?\",\n          \"What features make Google\\u2019s Palm API user-friendly for developers?\",\n          \"What makes large language models versatile?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"retrieved_contexts\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"response\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Large language models (LLMs) are deeply connected to deep learning as they are fundamentally based on neural network architectures that learn from extensive datasets. As explained in the video, LLMs utilize a large amount of training data\\u2014often up to petabytes\\u2014and can have billions of parameters which equip them to tackle various language tasks like translation and text classification (Context 1, 0:03:00 - 0:03:45). The deep learning framework enables these models to pre-train on general language tasks and then be fine-tuned for specific applications, maximizing their versatility and efficiency.\",\n          \"Google\\u2019s Palm API is user-friendly for developers primarily due to its integration with Maker Suite, as mentioned around 0:15:00 to 0:15:45. This integration provides a graphical user interface and various tools, including model training, deployment, and monitoring. The model training tool allows developers to train models using different algorithms, while the deployment tool offers various options to put models into production. Additionally, the monitoring tool provides performance insights through a user-friendly dashboard, making the entire process straightforward and efficient.\",\n          \"Large language models are versatile because they are initially pre-trained on massive datasets for general use and then fine-tuned on smaller, task-specific datasets. This allows a single model to perform a wide range of tasks such as language translation, text classification, and question answering with minimal additional training, as highlighted around 0:03:00 to 0:03:45. Their ability to adapt to both general and specific tasks makes them immensely valuable across various industries.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"reference\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"The text introduces a course on Large Language Models (LLMs) conducted by John Ewald at Google Cloud, covering definitions, use cases, prompt tuning, and Google's Genai tools, while linking LLMs to deep learning and generative AI.\",\n          \"Palm is a 540 billion parameter dense decoder-only transformer model, utilizing Google's Pathways system for efficient multi-task training and improved understanding through distributed computation.\",\n          \"Large language models are versatile AI systems designed to address various language tasks and can be specialized for specific industries by utilizing extensive training data and high parameter counts.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"factual_correctness\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.23638950907347814,\n        \"min\": 0.0,\n        \"max\": 0.57,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.09,\n          0.0,\n          0.43\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"faithfulness\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.19587887538751922,\n        \"min\": 0.18181818181818182,\n        \"max\": 0.5555555555555556,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.5555555555555556,\n          0.18181818181818182,\n          0.5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"semantic_similarity\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.041847203044516806,\n        \"min\": 0.8470818674048028,\n        \"max\": 0.9521312130475145,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.9004345264229277,\n          0.8470818674048028,\n          0.9410252249886943\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3fg8B0rnypf9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}